{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA-Factory QuickStart \n",
    "\n",
    "使用 LoRA + SFT 数据, 微调 Meta-Llama-3-8B-Instruct 在 4090 上. \n",
    "\n",
    "1. 原始模型直接推理\n",
    "2. 自定义数据集构建\n",
    "3. 基于LoRA的sft指令微调\n",
    "4. 动态合并LoRA的推理\n",
    "5. 批量预测和训练效果评估\n",
    "6. LoRA模型合并导出\n",
    "7. 一站式webui board的使用\n",
    "8. API Server的启动与调用\n",
    "9. 大模型主流评测 benchmark\n",
    "\n",
    "\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md  (先看这个)\n",
    "\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README.md\n",
    "\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README_zh.md\n",
    "\n",
    "https://github.com/CrazyBoyM/llama3-Chinese-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "! cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda create -n llama_factory python=3.10 & conda activate llama_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -e '.[torch,metrics]' \n",
    "\n",
    "# optional: torch、torch_npu、metrics、deepspeed、bitsandbytes、vllm、galore、badam、gptq、awq、aqlm、qwen、modelscope、quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! llamafactory-cli train -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct  # HuggingFace 需要认证\n",
    "\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "! git clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git\n",
    "\n",
    "# 要检查有没有下全"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果连不上 抱抱脸 \n",
    "\n",
    "```bash\n",
    "export USE_MODELSCOPE_HUB=1 # Windows 使用 `set USE_MODELSCOPE_HUB=1`\n",
    "```\n",
    "\n",
    "将 model_name_or_path 设置为模型 ID 来加载对应的模型。在 [魔搭社区](https://modelscope.cn/models) 查看所有可用的模型，例如 LLM-Research/Meta-Llama-3-8B-Instruct。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跑一下使用 transformer 的原始推理, 确认模型文件的完整, transformer 依赖都装好了\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# 切换为你下载的模型文件目录, 这里的demo是Llama-3-8B-Instruct\n",
    "# 如果是其他模型，比如qwen，chatglm，请使用其对应的官方demo\n",
    "model_id = \"/media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Llama-Factory 进行推理(WebUI)\n",
    "\n",
    "```bash\n",
    "llamafactory-cli webchat \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --template llama3\n",
    "```\n",
    "等效\n",
    "\n",
    "```bash\n",
    "llamafactory-cli webchat examples/inference/llama3.yaml\n",
    "\n",
    "这里 template 在这里找 https://github.com/hiyouga/LLaMA-Factory/tree/main/examples/inference```\n",
    "\n",
    "\n",
    "http://localhost:7860/\n",
    "\n",
    "阿里云用户需要做一下如下环境变量的配置才能正常运行gradio，然后再通过阿里云给的域名映射访问\n",
    "\n",
    "```bash\n",
    "export GRADIO_ROOT_PATH=/${JUPYTER_NAME}/proxy/7860/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llamafactory-cli 基本用法\n",
    "\n",
    "```bash\n",
    "llamafactory-cli version \n",
    "```\n",
    "\n",
    "```bash\n",
    "llamafactory-cli train \\ \n",
    "```\n",
    "\n",
    "```bash\n",
    "llamafactory-cli chat \\ \n",
    "```\n",
    "    \n",
    "模型合并和导出\n",
    "```bash \n",
    "llamafactory-cli export \\ \n",
    "```\n",
    "\n",
    "启动API server\n",
    "```bash \n",
    "llamafactory-cli api \\ \n",
    "```\n",
    "\n",
    "使用mmlu等标准数据集做评测\n",
    "```bash \n",
    "llamafactory-cli eval \\ \n",
    "```\n",
    "\n",
    "前端版本纯推理的chat页面\n",
    "```bash \n",
    "llamafactory-cli webchat \\ \n",
    "```\n",
    "\n",
    "\n",
    "启动LlamaBoard前端页面，包含可视化训练，预测，chat，模型合并多个子页面\n",
    "```bash \n",
    "llamafactory-cli webui \\ \n",
    "```\n",
    "\n",
    "\n",
    "参数的名称（huggingface或者modelscope上的标准定义，如“meta-llama/Meta-Llama-3-8B-Instruct”）， 或者是本地下载的绝对路径，如/media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \n",
    "```bash \n",
    "--model_name_or_path \n",
    "```\n",
    "\n",
    "模型问答时所使用的prompt模板，不同模型不同，请参考 https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#supported-models 获取不同模型的模板定义，否则会回答结果会很奇怪或导致重复生成等现象的出现。chat 版本的模型基本都需要指定，比如Meta-Llama-3-8B-Instruct的template 就是 llama3\n",
    "```bash\n",
    "-- template\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一站式webui board的使用**\n",
    "\n",
    "启动\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli webui\n",
    "```\n",
    "\n",
    "如果要开启 gradio的share功能，或者修改端口号\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 GRADIO_SHARE=1 GRADIO_SERVER_PORT=7860 llamafactory-cli webui\n",
    "```\n",
    "\n",
    "注意: 目前webui版本只支持单机单卡和单机多卡，如果是多机多卡请使用命令行版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  自定义数据集构建\n",
    "\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md\n",
    "\n",
    "\n",
    "`data/dataset_info.json` 包含了所有可用的数据集。如果您希望使用自定义数据集，请务必在 dataset_info.json 文件中添加数据集描述，并通过修改 dataset: 数据集名称 配置来使用数据集。\n",
    "\n",
    "目前支持 alpaca 格式和 sharegpt 格式的数据集。\n",
    "\n",
    "alpaca\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"人类指令（必填）\",\n",
    "    \"input\": \"人类输入（选填）\",\n",
    "    \"output\": \"模型回答（必填）\",\n",
    "    \"system\": \"系统提示词（选填）\",\n",
    "    \"history\": [\n",
    "      [\"第一轮指令（选填）\", \"第一轮回答（选填）\"],\n",
    "      [\"第二轮指令（选填）\", \"第二轮回答（选填）\"]\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "然后在 dataset_info.json 中注册\n",
    "\n",
    "```json\n",
    "\"数据集名称\": {\n",
    "  \"file_name\": \"data.json\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\",\n",
    "    \"system\": \"system\",\n",
    "    \"history\": \"history\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "支持 预训练 和 偏好数据集(DPO, ORPO, KTO(人类反馈 true/false), 多模态) 直接看 https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md\n",
    "\n",
    "sharegpt 有更多角色 human、gpt、observation、function (OpenAI 格式可以看作 sharegpt 的一种特殊形式)\n",
    "\n",
    "预训练不支持 ShareGPT 格式\n",
    "\n",
    "---\n",
    "\n",
    "一个关于中文 llama3 调教的 repo (star 3.2K)\n",
    "https://github.com/CrazyBoyM/llama3-Chinese-chat\n",
    "\n",
    "llama-factory 也包含了一些微调数据\n",
    "https://github.com/hiyouga/LLaMA-Factory/tree/main/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微调数据构建的注意事项\n",
    "- [Paper page - LIMA: Less Is More for Alignment](https://huggingface.co/papers/2305.11206) 样本质量远比数量重要, 而微调样本的质量主要关系如下维度:\n",
    "1. **样本多样性（Sample Diversity）**：\n",
    "\t1. **指令多样性**：考察样本中指令的覆盖范围是否广泛，是否包含了各类任务类型、不同难度级别以及多样化的指令结构和表达方式，确保模型在微调后能应对多种复杂情境。\n",
    "\t2. **内容多样性**：检查样本中提供的文本内容是否涵盖了不同主题、文体、长度以及语境，以避免模型在特定领域或文本类型上过拟合，确保其具备良好的泛化能力。\n",
    "2. **答案质量（Answer Quality）**：\n",
    "\t1. **准确性（Accuracy）**：评估答案是否准确无误地响应了给定指令和内容，是否忠实反映了任务要求，且不包含事实性错误、逻辑矛盾或语义模糊。\n",
    "\t2. **完备性（Completeness）**：考察答案是否全面覆盖了指令所要求的所有任务点，尤其对于多步骤或复合任务，答案应完整体现所有必要的操作结果。\n",
    "\t3. **简洁性与清晰度（Conciseness & Clarity）**：衡量答案是否言简意赅、表达清晰，避免冗余信息或含糊表述，确保模型在微调后生成的输出易于理解和使用。\n",
    "3. **一致性（Consistency）**：\n",
    "\t1. **内部一致性**：检查同一指令对不同内容的处理结果是否保持一致，即模型在相似情境下应给出相似的答案。\n",
    "\t2. **外部一致性**：对比样本答案与已知的知识库、专家判断或公认的基准结果，确保答案符合领域共识和常识。\n",
    "4. **难度适配（Difficulty Calibration）**：\n",
    "\t1. **难易程度分布**：分析样本集中简单、中等、复杂任务的比例，确保微调数据集包含不同难度级别的样本，有助于模型逐步提升处理复杂指令的能力。\n",
    "5. **噪声控制（Noise Reduction）**：\n",
    "\t1. **标签错误检查**：识别并剔除标注错误或不一致的样本，确保答案与指令、内容间的映射关系正确无误。\n",
    "\t2. **数据清洗**：去除重复样本、无关内容或低质量文本，提升数据集的整体纯净度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用我的项目自动化从txt文本中构建 instruct 数据集\n",
    "https://github.com/yoko19191/auto_instruct_data_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于LoRA 的微调\n",
    "\n",
    "不使用 LoRA 的话, 大概微调一个7B模型就需要60G显存, 使用 LoRA 16位精度 是 16G显存. QLoRA 4 位是 6G. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --dataset alpaca_gpt4_zh,identity,adgen_local \\\n",
    "    --dataset_dir ./data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir ./saves/LLaMA3-8B/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 50 \\\n",
    "    --warmup_steps 20 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --val_size 0.1 \\\n",
    "    --plot_loss \\\n",
    "    --fp16\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "| 参数名称                   | 参数说明                                                     |\n",
    "|----------------------------|----------------------------------------------------------|\n",
    "| stage                      | 当前训练的阶段，枚举值，有\"sft\",\"pt\",\"rw\",\"ppo\"等，代表了训练的不同阶段，这里我们是有监督指令微调，所以是sft |\n",
    "| do_train                   | 是否是训练模式                                             |\n",
    "| dataset                    | 使用的数据集列表，所有字段都需要按上文在data_info.json里注册，多个数据集用\",\"分隔 |\n",
    "| dataset_dir                | 数据集所在目录，这里是data，也就是项目自带的data目录        |\n",
    "| finetuning_type            | 微调训练的类型，枚举值，有\"lora\",\"full\",\"freeze\"等，这里使用lora |\n",
    "| output_dir                 | 训练结果保存的位置                                         |\n",
    "| cutoff_len                 | 训练数据集的长度截断                                       |\n",
    "| per_device_train_batch_size| 每个设备上的batch size，最小是1，如果GPU 显存够大，可以适当增加 |\n",
    "| fp16                       | 使用半精度混合精度训练                                     |\n",
    "| max_samples                | 每个数据集采样多少数据                                     |\n",
    "| val_size                   | 随机从数据集中抽取多少比例的数据作为验证集                  |\n",
    "\n",
    "注意：精度相关的参数还有bf16 和pure_bf16，但是要注意有的老显卡，比如V100就无法支持bf16，会导致程序报错或者其他错误\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完后就可以在设置的output_dir下看到如下内容，主要包含3部分\n",
    "\n",
    "adapter开头的就是 LoRA保存的结果了，后续用于模型推理融合\n",
    "\n",
    "training_loss 和trainer_log等记录了训练的过程指标\n",
    "\n",
    "其他是训练当时各种参数的备份"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态合并LoRA的推理\n",
    "\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/inference/llama3_lora_sft.yaml\n",
    "\n",
    "\n",
    "当基于LoRA的训练进程结束后，我们如果想做一下动态验证，在网页端里与新模型对话，与步骤4的原始模型直接推理相比，唯一的区别是需要通过finetuning_type参数告诉系统，我们使用了LoRA训练，然后将LoRA的模型位置通过 adapter_name_or_path参数即可\n",
    "\n",
    "\n",
    "```bash\n",
    "llamafactory-cli webchat \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --adapter_name_or_path ./saves/LLaMA3-8B/lora/sft  \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora\n",
    "```\n",
    "\n",
    "```bash\n",
    "llamafactory-cli chat \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --adapter_name_or_path ./saves/LLaMA3-8B/lora/sft  \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  批量预测和训练效果评估\n",
    "\n",
    "当然上文中的人工交互测试，会偏感性，那有没有办法批量地预测一批数据，然后使用自动化的bleu和 rouge等常用的文本生成指标来做评估。指标计算会使用如下3个库，请先做一下pip安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jieba\n",
    "! pip install rouge-chinese\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --adapter_name_or_path ./saves/LLaMA3-8B/lora/sft  \\\n",
    "    --dataset alpaca_gpt4_zh,identity,adgen_local \\\n",
    "    --dataset_dir ./data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir ./saves/LLaMA3-8B/lora/predict \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --max_samples 20 \\\n",
    "    --predict_with_generate\n",
    "```\n",
    "\n",
    "| 参数名称                   | 参数说明                                            |\n",
    "|----------------------------|----------------------------------------------------------|\n",
    "| do_predict                 | 现在是预测模式                                              |\n",
    "| predict_with_generate      | 现在用于生成文本                                            |\n",
    "| max_samples                | 每个数据集采样多少用于预测对比                              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "最后会在output_dir 输出评测结果\n",
    "\n",
    "其中 generated_predictions.jsonl 文件 输出了要预测的数据集的原始label和模型predict的结果\n",
    "\n",
    "predict_results.json给出了原始label和模型predict的结果，用自动计算的指标数据\n",
    "\n",
    "\n",
    "这里给相关的指标做一下进一步的解释\n",
    "\n",
    "\n",
    "好的，这是Markdown表格格式:\n",
    "\n",
    "| 指标 | 含义 |\n",
    "| --- | --- |\n",
    "| BLEU-4 | BLEU（Bilingual Evaluation Understudy）是一种常用的用于评估机器翻译质量的指标。BLEU-4 表示四元语法 BLEU 分数，它衡量模型生成文本与参考文本之间的 n-gram 匹配程度，其中 n=4。值越高表示生成的文本与参考文本越相似，最大值为 100。 |\n",
    "| predict_rouge-1 和 predict_rouge-2 | ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是一种用于评估自动摘要和文本生成模型性能的指标。ROUGE-1 表示一元 ROUGE 分数，ROUGE-2 表示二元 ROUGE 分数，分别衡量模型生成文本与参考文本之间的单个词和双词序列的匹配程度。值越高表示生成的文本与参考文本越相似，最大值为 100。 |\n",
    "| predict_rouge-l | ROUGE-L 衡量模型生成文本与参考文本之间最长公共子序列（Longest Common Subsequence）的匹配程度。值越高表示生成的文本与参考文本越相似，最大值为 100。 |\n",
    "| predict_runtime | 预测运行时间，表示模型生成一批样本所花费的总时间。单位通常为秒。 |\n",
    "| predict_samples_per_second | 每秒生成的样本数量，表示模型每秒钟能够生成的样本数量。通常用于评估模型的推理速度。 |\n",
    "| predict_steps_per_second | 每秒执行的步骤数量，表示模型每秒钟能够执行的步骤数量。对于生成模型，一般指的是每秒钟执行生成操作的次数。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA模型合并导出\n",
    "\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/merge_lora/llama3_lora_sft.yaml\n",
    "\n",
    "如果想把训练的LoRA和原始的大模型进行融合，输出一个完整的模型文件的话，可以使用如下命令。合并后的模型可以自由地像使用原始的模型一样应用到其他下游环节，当然也可以递归地继续用于训练。\n",
    "\n",
    "```bash\n",
    "llamafactory-cli export \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --adapter_name_or_path ./saves/LLaMA3-8B/lora/sft  \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --export_dir megred-model-path \\\n",
    "    --export_size 2 \\\n",
    "    --export_device cpu \\\n",
    "    --export_legacy_format False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Server的启动与调用\n",
    "\n",
    "\n",
    "训练好后，可能部分同学会想将模型的能力形成一个可访问的网络接口，通过API 来调用，接入到langchian或者其他下游业务中，项目也自带了这部分能力。\n",
    "\n",
    "API 实现的标准是参考了OpenAI的相关接口协议，基于uvicorn服务框架进行开发， 使用如下的方式启动\n",
    "\n",
    "```bash\n",
    "API_PORT=8000 llamafactory-cli api \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --adapter_name_or_path ./saves/LLaMA3-8B/lora/sft \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora\n",
    "```\n",
    "\n",
    "\n",
    "项目也支持了基于vllm 的推理后端，但是这里由于一些限制，需要提前将LoRA 模型进行merge，使用merge后的完整版模型目录或者训练前的模型原始目录都可。\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 API_PORT=8000 llamafactory-cli api \\\n",
    "    --model_name_or_path megred-model-path \\\n",
    "    --template llama3 \\\n",
    "    --infer_backend vllm \\\n",
    "    --vllm_enforce_eager\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务启动后，即可按照openai 的API 进行远程访问，主要的区别就是替换 其中的base_url，指向所部署的机器url和端口号即可。\n",
    "\n",
    "```python\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "require_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # change to your custom port\n",
    "    port = 8000\n",
    "    client = OpenAI(\n",
    "        api_key=\"0\",\n",
    "        base_url=\"http://localhost:{}/v1\".format(os.environ.get(\"API_PORT\", 8000)),\n",
    "    )\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": \"hello, where is USA\"})\n",
    "    result = client.chat.completions.create(messages=messages, model=\"test\")\n",
    "    print(result.choices[0].message)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对模型进行量化 (GGUF, GPTQ, AWQ, EXL2)\n",
    "\n",
    "\n",
    "https://huggingface.co/blog/zh/hf-bitsandbytes-integration\n",
    "\n",
    "Safetensors 和 PyTorch 二进制文件是原始 float16 模型文件的示例。这些文件主要用于持续微调目的。\n",
    "\n",
    "GGML 与 GGUF 指代同一概念，其中 GGUF 作为较新版本，融入了关于模型的额外数据。此项增强功能支持多种架构，并包含提示模板。GGUF 可仅在 CPU 上执行，或部分/完全卸载至 GPU。借助 K 量化技术，GGUF 的位宽可从 2 位扩展至 8 位。\n",
    "\n",
    "\n",
    "先前，GPTQ 作为一种仅针对 GPU 优化的量化方法，但已被 AWQ 超越，后者速度约快两倍。该领域最新进展是 EXL2，其性能更佳。通常，这些量化方法采用 4 位实现\n",
    "\n",
    "我们可能需要把微调后的进行量化和模型转为 GGUF 格式, 方便 Ollama llama.cpp 的部署推理. \n",
    "\n",
    "用这个可以快速吧HuggingFace 上的模型改为GGUF格式\n",
    "https://huggingface.co/spaces/ggml-org/gguf-my-repo\n",
    "\n",
    "\n",
    "或者使用 llama.cpp 进行量化\n",
    "\n",
    "(没写完)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进阶-大模型主流评测 benchmark\n",
    "\n",
    "虽然大部分同学的主流需求是定制一个下游的垂直模型，但是在部分场景下，也可能有同学会使用本项目来做更高要求的模型训练，用于大模型刷榜单等，比如用于评测mmlu等任务。当然这类评测同样可以用于评估大模型二次微调之后，对于原来的通用知识的泛化能力是否有所下降。（因为一个好的微调，尽量是在具备垂直领域知识的同时，也保留了原始的通用能力）\n",
    "\n",
    "本项目提供了mmlu，cmmlu, ceval三个常见数据集的自动评测脚本，按如下方式进行调用即可\n",
    "\n",
    "\n",
    "如果是Chat model \n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli eval \\\n",
    "--model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "--template llama3 \\\n",
    "--task mmlu \\\n",
    "--split validation \\\n",
    "--lang en \\\n",
    "--n_shot 5 \\\n",
    "--batch_size 1\n",
    "```\n",
    "\n",
    "\n",
    "可能的输出如下, 具体任务的指标定义请参考mmlu，cmmlu, ceval等任务原始的相关资料, 和llama3的官方报告基本一致\n",
    "\n",
    "```text\n",
    "Average: 63.64                                                                                                                                     \n",
    "           STEM: 50.83\n",
    "Social Sciences: 76.31\n",
    "     Humanities: 56.63\n",
    "          Other: 73.31\n",
    "\n",
    "```\n",
    "\n",
    "如果是base版本的模型，template改为fewshot即可\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli eval \\\n",
    "--model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B \\\n",
    "--template fewshot \\\n",
    "--task mmlu \\\n",
    "--split validation \\\n",
    "--lang en \\\n",
    "--n_shot 5 \\\n",
    "--batch_size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
