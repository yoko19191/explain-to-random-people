{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接下载 `safetensors` 文件然后用对外服务 chat completion\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/702475296\n",
    "\n",
    "https://modelscope.cn/models/qwen/Qwen2-7B-Instruct/files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, Response\n",
    "from flask_cors import CORS\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the tokenizer and model once when the application starts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "@app.route('/v1/chat/completions', methods=['POST'])\n",
    "def chat_completions():\n",
    "    data = request.json\n",
    "    messages = data.get('messages', [])\n",
    "    top_k = data.get('top_k', None)\n",
    "    top_p = data.get('top_p', 1)\n",
    "    temperature = data.get('temperature', 0.3)\n",
    "    max_length = data.get('max_length', 2500)\n",
    "    logging.info(f\"Received data: {json.dumps(data, ensure_ascii=False)}\")  # Correctly formatted logging to print request data\n",
    "    inputs = tokenizer.apply_chat_template(messages,\n",
    "                                           add_generation_prompt=True,\n",
    "                                           tokenize=True,\n",
    "                                           return_tensors=\"pt\",\n",
    "                                           return_dict=True\n",
    "                                           )\n",
    "    logging.info(f\"messages: {messages}\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "  \n",
    "    gen_kwargs = {\n",
    "        \"max_length\": max_length,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": top_p,\n",
    "        \"temperature\":temperature\n",
    "    }\n",
    "\n",
    "\n",
    "    # 这个不是真流式，能同时兼容 glm4 和 qwen2\n",
    "    def generate_response():\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **gen_kwargs)\n",
    "            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            for char in response_text:\n",
    "                logging.info(f\"Sending character: {char}\")\n",
    "                yield f\"data: {json.dumps({'choices': [{'delta': {'content': char}}]})}\\n\\n\"\n",
    "            # yield f\"data: {json.dumps({'choices': [{'delta': {'content': '[DONE]'}}]})}\\n\\n\"\n",
    "  \n",
    "    return Response(generate_response(), content_type='text/event-stream')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='127.0.0.1', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
